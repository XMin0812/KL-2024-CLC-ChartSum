{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tải thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"./llama-recipes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121\n",
    "!pip install -U pip setuptools\n",
    "!pip install --extra-index-url https://download.pytorch.org/whl/test/cu118 -e .\n",
    "!pip install -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tiền xử lý dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Xử lý dữ liệu theo format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../preprocessDataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Dịch tập ngữ liệu sang tiếng việt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../translateDataset.py --batch_size 8 --file_path_input ./data/train_valid.json  --file_path_output ./llama-recipes/src/llama_recipes/datasets/statista_data.json\n",
    "!python ../translateDataset.py --batch_size 8 --file_path_input ./data/test.json  --file_path_output ./data/test_vn.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Huấn luyện mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('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')\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "```php\n",
    "Lựa chọn 1 trong 2 mô hình để chạy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mô hình LLaMA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Base_model = \"meta-llama/Meta-Llama-2-7B\"\n",
    "Improved_model = \"XMin08/Model_Llama2_new_v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mô hình LLaMA3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Base_model = \"meta-llama/Meta-Llama-3-8B\"\n",
    "Improved_model = \"XMin08/Model_Llama3_new_v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 3.1 Huấn luyện mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m llama_recipes.finetuning --context_length 1 --model_name Base_model --use_peft --peft_method lora --quantization --batch_size_training 1 --num_epochs 3 --use_fp16 --save_metrics --output_dir \"../model_adapter/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Kết hợp mô hình gốc và đưa lên huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "lora_weights = \"../model_adapter/\"\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "        Base_model,\n",
    "        load_in_8bit=False,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        offload_folder=\"tmp\")\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    Base_model\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    lora_weights,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=\"tmp\",\n",
    ")\n",
    "\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "model.push_to_hub(Improved_model)\n",
    "tokenizer.push_to_hub(Improved_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Đánh giá văn bản"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Tải mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "lora_weights = \"../model_adapter/\"\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(Base_model)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    Base_model,\n",
    "    load_in_8bit=False,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    lora_weights,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
    "model.config.bos_token_id = 1\n",
    "model.config.eos_token_id = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Quá trình sinh câu dự đoán cho việc tóm tắt văn bản"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "from random import randint\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "\n",
    "with open(\"../data/test_vn.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    dataset = json.load(file)\n",
    "\n",
    "prompt = \"\"\"\n",
    "              Below is an instruction that describes a task, paired with an input that provides further context.\n",
    "              Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\\n{}\\n\\n### Input:\\n{}\\n\\n### Response:\n",
    "\"\"\"\n",
    "Predicts = []\n",
    "References = []\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    top_k=40,\n",
    "    num_beams=4,\n",
    "    do_sample = True\n",
    ")\n",
    "\n",
    "for index, item in enumerate(dataset):\n",
    "    print(f\"Processing item index: {index}\")\n",
    "    prompt_sentence = prompt.format(item['instruction'], item['input'])\n",
    "    inputs = tokenizer(prompt_sentence, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(\"cuda\")\n",
    "    generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=512,\n",
    "        )\n",
    "    s = generation_output.sequences[0]\n",
    "    output = tokenizer.decode(s)\n",
    "    predict = output.split(\"### Response:\")[1].strip().replace(\"</s>\", \"\")\n",
    "    Predicts.append(predict)\n",
    "    References.append(item['output'])\n",
    "\n",
    "with open(\"../result/Predictions.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(Predicts, file, ensure_ascii=False, indent=2)\n",
    "with open(\"../result/References.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(References, file, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Điểm Bleu và Rouge của kết quả dự đoán của mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load('rouge')\n",
    "results_bleu = bleu.compute(predictions = Predicts, references = References)\n",
    "results_rouge = rouge.compute(predictions = Predicts, references = References)\n",
    "print(\"Bleu score : \", results_bleu)\n",
    "print(\"Rouge score: \", results_rouge)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
